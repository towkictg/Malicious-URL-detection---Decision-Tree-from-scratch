# -*- coding: utf-8 -*-
"""url detection new method.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iMBxsD3KDivSa8ifZAtE7rwP8evZ4RMw
"""

import pandas as pd
import numpy as np
import random


# Machine Learning Packages
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Load Url Data
urlsdata = pd.read_csv("urldata.csv")

type(urlsdata)

urldata = urlsdata.sample(frac=0.1, random_state=42)  # random_state for reproducibility

# Optional: Reset the index if you want a clean index for the new DataFrame
urldata.reset_index(drop=True, inplace=True)

# Display the sampled data
print(urldata)

urldata.shape

urldata.head()
#urls_data.shape
#urls_data_subset = urls_data.head(1000)

!pip install tld

#Importing dependencies
from urllib.parse import urlparse
from tld import get_tld
import os.path

#Length of URL
urldata['url_length'] = urldata['url'].apply(lambda i: len(str(i)))

#Hostname Length
urldata['hostname_length'] = urldata['url'].apply(lambda i: len(urlparse(i).netloc))

#Path Length
urldata['path_length'] = urldata['url'].apply(lambda i: len(urlparse(i).path))

#First Directory Length
def fd_length(url):
    urlpath= urlparse(url).path
    try:
        return len(urlpath.split('/')[1])
    except:
        return 0

urldata['fd_length'] = urldata['url'].apply(lambda i: fd_length(i))

#Length of Top Level Domain
urldata['tld'] = urldata['url'].apply(lambda i: get_tld(i,fail_silently=True))
def tld_length(tld):
    try:
        return len(tld)
    except:
        return -1

urldata['tld_length'] = urldata['tld'].apply(lambda i: tld_length(i))

urldata.head()

urldata = urldata.drop("tld", axis=1)

urldata.head()

urldata['count-'] = urldata['url'].apply(lambda i: i.count('-'))

urldata['count@'] = urldata['url'].apply(lambda i: i.count('@'))

urldata['count?'] = urldata['url'].apply(lambda i: i.count('?'))

urldata['count%'] = urldata['url'].apply(lambda i: i.count('%'))

urldata['count.'] = urldata['url'].apply(lambda i: i.count('.'))

urldata['count='] = urldata['url'].apply(lambda i: i.count('='))

urldata['count-http'] = urldata['url'].apply(lambda i : i.count('http'))

urldata['count-https'] = urldata['url'].apply(lambda i : i.count('https'))

urldata['count-www'] = urldata['url'].apply(lambda i: i.count('www'))

def digit_count(url):
    digits = 0
    for i in url:
        if i.isnumeric():
            digits = digits + 1
    return digits
urldata['count-digits']= urldata['url'].apply(lambda i: digit_count(i))

def letter_count(url):
    letters = 0
    for i in url:
        if i.isalpha():
            letters = letters + 1
    return letters
urldata['count-letters']= urldata['url'].apply(lambda i: letter_count(i))

def no_of_dir(url):
    urldir = urlparse(url).path
    return urldir.count('/')
urldata['count_dir'] = urldata['url'].apply(lambda i: no_of_dir(i))

urldata.shape

import re

#Use of IP or not in domain
def having_ip_address(url):
    match = re.search(
        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4
        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)' # IPv4 in hexadecimal
        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6
    if match:
        # print match.group()
        return -1
    else:
        # print 'No matching pattern found'
        return 1
urldata['use_of_ip'] = urldata['url'].apply(lambda i: having_ip_address(i))

def shortening_service(url):
    match = re.search('bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|'
                      'yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|'
                      'short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|'
                      'doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|'
                      'db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|'
                      'q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|'
                      'x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|''tr\.im|link\.zip\.net',
                      url)
    if match:
        return -1
    else:
        return 1
urldata['short_url'] = urldata['url'].apply(lambda i: shortening_service(i))

urldata.head()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

from sklearn.tree import DecisionTreeClassifier

from sklearn.linear_model import LogisticRegression
#Predictor Variables
x = urldata[['hostname_length',
       'path_length', 'fd_length', 'tld_length', 'count-', 'count@', 'count?',
       'count%', 'count.', 'count=', 'count-http','count-https', 'count-www', 'count-digits',
       'count-letters', 'count_dir', 'use_of_ip']]

#Target Variable
y = urldata['label']

x.shape
y.shape

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.3, random_state=42)

import numpy as np
import pandas as pd

class DecisionTree:
    """A simple Decision Tree Classifier."""

    class Node:
        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
            self.feature = feature  # Index of the feature to split on
            self.threshold = threshold  # Value of the threshold
            self.left = left  # Left subtree
            self.right = right  # Right subtree
            self.value = value  # Class label if it's a leaf node

    def __init__(self, min_samples_split=2, max_depth=float('inf')):
        self.min_samples_split = min_samples_split
        self.max_depth = max_depth
        self.tree = None

    def fit(self, X, y):
        """Fit the decision tree to the provided training data."""
        X = X.values if isinstance(X, pd.DataFrame) else X
        y = y.values if isinstance(y, pd.Series) else y
        self.tree = self._build_tree(X, y)

    def _build_tree(self, X, y, depth=0):
        num_samples = len(y)

        # Check if we should stop splitting
        if num_samples < self.min_samples_split or depth >= self.max_depth:
            return self.Node(value=self._most_common_label(y))

        best_feature, best_threshold = self._best_split(X, y)
        if best_feature is None:
            return self.Node(value=self._most_common_label(y))

        left_indices = [i for i in range(num_samples) if X[i][best_feature] < best_threshold]
        right_indices = [i for i in range(num_samples) if X[i][best_feature] >= best_threshold]

        # Prevent recursion if left or right indices are empty
        if not left_indices or not right_indices:
            return self.Node(value=self._most_common_label(y))

        left_child = self._build_tree(X[left_indices], y[left_indices], depth + 1)
        right_child = self._build_tree(X[right_indices], y[right_indices], depth + 1)

        return self.Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)

    def _best_split(self, X, y):
        best_gini = float('inf')
        best_feature = None
        best_threshold = None
        num_features = X.shape[1]

        for feature in range(num_features):
            thresholds = sorted(set(X[:, feature]))
            for threshold in thresholds:
                gini = self._gini_index(X, y, feature, threshold)
                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def _gini_index(self, X, y, feature_index, threshold):
        left_indices = [i for i in range(len(y)) if X[i][feature_index] < threshold]
        right_indices = [i for i in range(len(y)) if X[i][feature_index] >= threshold]

        gini_left = self._gini(y, left_indices)
        gini_right = self._gini(y, right_indices)

        weighted_gini = (len(left_indices) * gini_left + len(right_indices) * gini_right) / len(y)
        return weighted_gini

    def _gini(self, y, indices):
        if not indices:
            return 0
        label_counts = {}
        for i in indices:
            label_counts[y[i]] = label_counts.get(y[i], 0) + 1
        impurity = 1.0
        for count in label_counts.values():
            prob = count / len(indices)
            impurity -= prob ** 2
        return impurity

    def _most_common_label(self, y):
        if len(y) == 0:
            return None
        label_count = {}
        for label in y:
            label_count[label] = label_count.get(label, 0) + 1
        return max(label_count, key=label_count.get)

    def predict(self, X):
        """Predict class labels for samples in X."""
        X = X.values if isinstance(X, pd.DataFrame) else X
        return [self._predict(sample) for sample in X]

    def _predict(self, sample):
        node = self.tree
        while node.value is None:
            if sample[node.feature] < node.threshold:
                node = node.left
            else:
                node = node.right
        return node.value


# Example usage
if __name__ == "__main__":
    # Assuming x_train, y_train, and x_test are defined as either NumPy arrays or pandas DataFrames/Series
    # Create the Decision Tree Classifier
    dt_model = DecisionTree()

    # Fit the model with your existing training data
    dt_model.fit(x_train, y_train)

# Make predictions on new data
    predictions = dt_model.predict(x_test)

    # Output predictions
    print("Predictions:", predictions)

# Assuming y_test is the actual labels for x_test
correct_predictions = sum(1 for true, pred in zip(y_test, predictions) if true == pred)
accuracy = correct_predictions / len(y_test)

# Calculate precision (assuming binary classification with 1 as the positive class)
true_positive = sum(1 for true, pred in zip(y_test, predictions) if true == pred == 1)
predicted_positive = sum(1 for pred in predictions if pred == 1)
precision = true_positive / predicted_positive if predicted_positive > 0 else 0

print("Zip:", list(zip(y_test, predictions)))


print("Accuracy:", accuracy)
print("Precision:", precision)

from sklearn.metrics import confusion_matrix

# Calculate confusion matrix
cm = confusion_matrix(y_test, predictions)
print("Confusion Matrix:")
print(cm)

































